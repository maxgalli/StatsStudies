{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Method: Mean of a Gaussian\n",
    "(inspired by [this](https://matthewfeickert.github.io/Statistics-Notes/notebooks/Introductory/Likelihood-Function.html) tutorial)\n",
    "\n",
    "In section () we have seen how the likelihood function is defined:\n",
    "\n",
    "$$L(\\theta)=f(x_1|\\theta)\\cdot f(x_2|\\theta)\\cdots f(x_n|\\theta)=\\prod_{i = 1}^{Nevts} f(x_i|\\theta).$$\n",
    "\n",
    "We then noticed how, for convenience, in HEP we use the negative log-likelihood:\n",
    "\n",
    "$$NLL(\\theta) = - \\sum_{i = 1}^{Nevts} \\text{ln} f(x_i|\\theta)$$\n",
    "\n",
    "for which we find the minimum.\n",
    "\n",
    "In section () we then saw how the uncertainty of a ML estimator at $\\pm 1 \\sigma$ can be found by checking where the the NLL increases from the maximum by 0.5.\n",
    "\n",
    "Here we will apply these easy concepts to a simple case: the estimation of the mean of a gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood and NLL can be written like follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Likelihood:\n",
    "    function: callable\n",
    "    data: np.ndarray\n",
    "\n",
    "    def __call__(self, *params):\n",
    "        return np.prod(self.function(self.data, *params))\n",
    "\n",
    "\n",
    "class NLL(Likelihood):\n",
    "    def __call__(self, *params):\n",
    "        return -np.sum([np.log(self.function(self.data, *params))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where in the signature of ```__call__``` every reference to data has been purposely avoided to emphasize the fact that the likelihood is a function of parameters only.\n",
    "\n",
    "Our gaussian function can be written as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mu, sigma):\n",
    "    return (\n",
    "        1 / (sigma * np.sqrt(2 * np.pi)) * np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true mean: 0.006\n",
      "true standard deviation: 0.402\n"
     ]
    }
   ],
   "source": [
    "true_mu = np.random.uniform(-0.5, 0.5)\n",
    "true_sigma = np.random.uniform(0.1, 1.0)\n",
    "\n",
    "print(f\"true mean: {true_mu:.3f}\")\n",
    "print(f\"true standard deviation: {true_sigma:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a1024940c344a7995b242adc0b37d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6f78a3bfb04611a5469c0022a90a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='n_samples', max=200, min=1), Output()), _dom_classes=('…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 5))\n",
    "\n",
    "@interact(n_samples=widgets.IntSlider(min=1, max=200, step=1, value=50))\n",
    "def draw(n_samples):\n",
    "    axes[0, 0].clear()\n",
    "    axes[1, 0].clear()\n",
    "\n",
    "    lkl_label = r\"$L\\left(\\mu | \\vec{x}\\right)$\"\n",
    "    nll_label = r\"$NLL\\left(\\mu | \\vec{x}\\right)$\"\n",
    "\n",
    "    fig.suptitle(f\"{n_samples} samples\")\n",
    "\n",
    "    samples = np.random.normal(true_mu, true_sigma, n_samples)\n",
    "    lkl = Likelihood(gaussian, samples)\n",
    "    nll = NLL(gaussian, samples)\n",
    "\n",
    "    n_mu  = 1000\n",
    "    mus = np.linspace(-0.5, 0.5, n_mu)\n",
    "    sigma = true_sigma\n",
    "\n",
    "    lkl_scan = np.array([lkl(mu, sigma) for mu in mus])\n",
    "    nll_scan = np.array([nll(mu, sigma) for mu in mus])\n",
    "    idx_min_nll_scan = np.argmin(nll_scan)\n",
    "    mu_best = mus[idx_min_nll_scan]\n",
    "    nll_scan -= nll_scan[idx_min_nll_scan] # move minimum to 0\n",
    "    arr_min_nll_ = np.ones(n_mu)*0.5\n",
    "    low_idx, high_idx = np.argwhere(np.diff(np.sign(nll_scan - arr_min_nll_))).flatten()\n",
    "    unc = np.abs(mus[high_idx] - mus[low_idx])\n",
    "    mu_low = abs(mu_best - mus[low_idx])\n",
    "    mu_high = abs(mu_best - mus[high_idx])\n",
    "\n",
    "    axes[0, 0].plot(mus, lkl_scan, label=lkl_label)\n",
    "    axes[0, 0].set_xlim(-0.5, 0.5)\n",
    "    axes[0, 0].plot(samples, np.zeros_like(samples), \"k|\", label=r\"$\\vec{x}$\")\n",
    "    axes[0, 0].set_ylabel(lkl_label)\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[1, 0].plot(mus, nll_scan, label=nll_label)\n",
    "    axes[1, 0].set_xlim(-0.5, 0.5)\n",
    "    axes[1, 0].set_ylim(0, 3)\n",
    "    axes[1, 0].set_ylabel(nll_label)\n",
    "    axes[1, 0].set_xlabel(r\"$\\mu$\")\n",
    "    axes[1, 0].axvline(mu_best, color=\"k\", linestyle=\"--\", label=r\"$\\mu_{best}$\")\n",
    "    axes[1, 0].axvline(true_mu, color=\"r\", linestyle=\"--\", label=r\"$\\mu_{true}$\")\n",
    "    axes[1, 0].axhline(0.5, color=\"k\", linestyle=\"--\", linewidth=0.5)\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "    axes[1, 1].plot(n_samples, unc, 'ro')\n",
    "    axes[1, 1].set_xlim(0, 200)\n",
    "    axes[1, 1].set_ylim(0, 0.5)\n",
    "    axes[1, 1].set_ylabel(\"Uncertainty\")\n",
    "    axes[1, 1].set_xlabel(r\"$N_{samples}$\")\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot on the right (and be looking at how the NLL width shrinks) you can notice see how the uncertainty on the estimated parameter decreases as a function of the number of samples (i.e., more statistics implies more precise estimations)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69ca9f6dd25aad5fc7b31625bcfbde310e390cf4c052d14668182af17b0fee34"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('stats-studies': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
